# ğŸŒ Kafka Streaming Weather Analytics Pipeline

## ğŸ“‹ Vue d'Ensemble

Ce projet implÃ©mente un pipeline complet d'analyse de donnÃ©es mÃ©tÃ©o en temps rÃ©el utilisant Apache Kafka, Spark Structured Streaming, et des technologies de Big Data. Le systÃ¨me couvre toute la chaÃ®ne de traitement depuis l'ingestion des donnÃ©es jusqu'Ã  la visualisation business intelligence.

## ğŸ—ï¸ Architecture Globale

```
ğŸ“¡ APIs MÃ©tÃ©o (Open-Meteo)
    â†“
ğŸ”„ Kafka Producers (Python)
    â†“
ğŸ“Š Kafka Topics (weather_stream, geo_weather_stream, etc.)
    â†“
âš¡ Spark Structured Streaming (Transformations)
    â†“
ğŸ“ˆ Aggregations temps rÃ©el (Windows, Watermarks)
    â†“
ğŸ—„ï¸ HDFS Storage (Structure gÃ©ographique)
    â†“
ğŸ“Š Visualisations & Analytics (Dashboard BI)
```

## ğŸ“‚ Structure du Projet

```
kafka/
â”œâ”€â”€ exercices/
â”‚   â”œâ”€â”€ exercice1/     # ğŸ”§ Kafka Setup & Topics
â”‚   â”œâ”€â”€ exercice2/     # ğŸ Python Consumer
â”‚   â”œâ”€â”€ exercice3/     # ğŸŒ¤ï¸ Weather API Producer  
â”‚   â”œâ”€â”€ exercice4/     # âš¡ Spark Transformations
â”‚   â”œâ”€â”€ exercice5/     # ğŸ“ˆ Real-time Aggregations
â”‚   â”œâ”€â”€ exercice6/     # ğŸ—ºï¸ Geo-Weather Producer
â”‚   â”œâ”€â”€ exercice7/     # ğŸ—„ï¸ HDFS Consumer
â”‚   â””â”€â”€ exercice8/     # ğŸ“Š Data Visualizations
â”œâ”€â”€ kafka_2.13-3.9.1/  # Apache Kafka Installation
â””â”€â”€ README.md          # Ce fichier
```

## ğŸš€ Exercices ImplÃ©mentÃ©s

### 1ï¸âƒ£ Exercice 1: Kafka Foundation
**Objectif**: Configuration Kafka et manipulation topics
- âœ… Scripts PowerShell pour Windows
- âœ… CrÃ©ation topics automatisÃ©e
- âœ… Envoi/lecture messages CLI
- âœ… Configuration KRaft mode

**Technologies**: Apache Kafka 2.13-3.9.1, PowerShell
**Fichiers clÃ©s**: `start-kafka-exercice1.ps1`, `create-topic.ps1`

### 2ï¸âƒ£ Exercice 2: Python Consumer
**Objectif**: Consommateur Python avec CLI interactive
- âœ… KafkaConsumerApp class avec argparse
- âœ… Gestion signaux (Ctrl+C) propre
- âœ… Formatting emojis et JSON pretty-print
- âœ… Consumer groups et offset management

**Technologies**: kafka-python, argparse, signal handling
**Fichiers clÃ©s**: `consumer.py`

### 3ï¸âƒ£ Exercice 3: Weather API Producer
**Objectif**: Producteur mÃ©tÃ©o temps rÃ©el avec API externe
- âœ… WeatherProducer class avec Open-Meteo API
- âœ… Streaming continu avec intervalle configurable
- âœ… Gestion erreurs et retry automatique
- âœ… CoordonnÃ©es Bordeaux et donnÃ©es enrichies

**Technologies**: Open-Meteo API, requests, JSON streaming
**Fichiers clÃ©s**: `current_weather.py`

### 4ï¸âƒ£ Exercice 4: Spark Transformations
**Objectif**: Transformations Spark avec calculs d'alertes
- âœ… WeatherAlertProcessor avec Structured Streaming
- âœ… Calculs niveaux alerte (vent/chaleur) automatiques
- âœ… Schema JSON complexe et transformation colonnes
- âœ… Topic sortie `weather_transformed`

**Technologies**: PySpark, Structured Streaming, Complex JSON
**Fichiers clÃ©s**: `weather_alerts.py`

### 5ï¸âƒ£ Exercice 5: Real-time Aggregations
**Objectif**: AgrÃ©gations temps rÃ©el avec fenÃªtres glissantes
- âœ… WeatherAggregatesProcessor avec dual streams
- âœ… Sliding windows 5 minutes avec watermark 2 minutes
- âœ… AgrÃ©gations temporelles ET rÃ©gionales parallÃ¨les
- âœ… MÃ©triques avancÃ©es (avg, max, min, count distinct)

**Technologies**: Spark Windows, Watermarking, approx_count_distinct
**Fichiers clÃ©s**: `weather_aggregates.py`

### 6ï¸âƒ£ Exercice 6: Geo-Weather Producer
**Objectif**: Producteur gÃ©olocalisÃ© avec API gÃ©ocodage
- âœ… GeoWeatherProducer avec geocoding API Open-Meteo
- âœ… Arguments ville/pays CLI intuitifs
- âœ… Enrichissement donnÃ©es gÃ©ographiques (timezone, population)
- âœ… ClÃ©s Kafka structurÃ©es pour partitioning gÃ©ographique

**Technologies**: Open-Meteo Geocoding API, Enhanced JSON
**Fichiers clÃ©s**: `geo_weather.py`

### 7ï¸âƒ£ Exercice 7: HDFS Consumer
**Objectif**: Sauvegarde organisÃ©e par structure gÃ©ographique
- âœ… HDFSWeatherConsumer avec partitioning automatique
- âœ… Structure `/hdfs-data/{country}/{city}/alerts.json`
- âœ… JSON Lines format pour append efficient
- âœ… MÃ©tadonnÃ©es traÃ§abilitÃ© et consumer groups

**Technologies**: Kafka Consumer, File I/O, Geographic partitioning
**Fichiers clÃ©s**: `hdfs_consumer.py`

### 8ï¸âƒ£ Exercice 8: Data Visualizations
**Objectif**: Dashboard analytics et business intelligence
- âœ… HDFSWeatherAnalyzer avec 7 types visualisations
- âœ… Dashboard complet: tempÃ©rature, vent, alertes, gÃ©ographie
- âœ… Rapport HTML interactif responsive
- âœ… MÃ©triques automatisÃ©es et codes mÃ©tÃ©o WMO

**Technologies**: pandas, matplotlib, seaborn, HTML dashboard
**Fichiers clÃ©s**: `weather_visualizer.py`, rapport HTML

## ğŸ“Š Pipeline de DonnÃ©es Complet

### 1. Ingestion
```bash
# DÃ©marrer Kafka
./start-kafka.bat

# GÃ©nÃ©rer donnÃ©es gÃ©olocalisÃ©es
python exercice6/geo_weather.py "Paris" "France" --topic geo_weather_stream
```

### 2. Transformation
```bash
# Lancer transformations Spark
python exercice4/weather_alerts.py
python exercice5/weather_aggregates.py
```

### 3. Storage
```bash
# Sauvegarder en HDFS
python exercice7/hdfs_consumer.py --hdfs-path ./hdfs-data
```

### 4. Analytics
```bash
# GÃ©nÃ©rer visualisations
python exercice8/weather_visualizer.py --hdfs-path ./hdfs-data --report
```

## ğŸ› ï¸ Installation & Configuration

### PrÃ©requis
- **Java 8+** (pour Kafka)
- **Python 3.7+** avec pip
- **Windows PowerShell** (scripts optimisÃ©s)
- **AccÃ¨s Internet** (APIs Open-Meteo)

### Setup Rapide
```powershell
# 1. Cloner le projet
git clone <repository>
cd kafka

# 2. DÃ©marrer Kafka
.\start-kafka.bat

# 3. Installer dÃ©pendances Python par exercice
cd exercices\exercice2
pip install -r requirements.txt

# 4. Tester un exercice
python consumer.py weather_stream
```

### Configuration Kafka
- **Mode**: KRaft (pas de ZooKeeper)
- **Port**: 9092
- **Logs**: `C:\tmp\kraft-combined-logs`
- **Topics**: Auto-crÃ©ation activÃ©e

## ğŸ“ˆ MÃ©triques & Performance

### Throughput Typique
- **Producteur mÃ©tÃ©o**: ~1 message/seconde/ville
- **Consommateur**: ~100 messages/seconde
- **Spark Streaming**: ~50 messages/seconde transformÃ©s
- **HDFS Writer**: ~10MB/heure pour 10 villes

### Resources
- **Kafka**: ~200MB RAM, 1 CPU core
- **Spark**: ~512MB RAM, 2 CPU cores
- **Python apps**: ~50MB RAM chacune
- **Stockage**: ~1KB par message mÃ©tÃ©o

## ğŸŒ APIs Externes

### Open-Meteo Weather API
- **Endpoint**: `https://api.open-meteo.com/v1/forecast`
- **Rate Limit**: Gratuit illimitÃ©
- **DonnÃ©es**: TempÃ©rature, vent, codes mÃ©tÃ©o WMO

### Open-Meteo Geocoding API
- **Endpoint**: `https://geocoding-api.open-meteo.com/v1/search`
- **FonctionnalitÃ©**: RÃ©solution ville/pays â†’ coordonnÃ©es
- **Enrichissement**: Timezone, population, rÃ©gions

## ğŸ”„ Workflow Git

Le projet utilise une stratÃ©gie de branches par exercice:

```bash
# Structure branches
master              # Branch principale avec README
â”œâ”€â”€ exercice1       # Kafka setup
â”œâ”€â”€ exercice2       # Python consumer
â”œâ”€â”€ exercice3       # Weather producer
â”œâ”€â”€ exercice4       # Spark transformations
â”œâ”€â”€ exercice5       # Aggregations
â”œâ”€â”€ exercice6       # Geo-producer
â”œâ”€â”€ exercice7       # HDFS consumer
â””â”€â”€ exercice8       # Visualizations
```

### Commandes Git Utiles
```bash
# Voir toutes les branches
git branch -a

# Basculer vers un exercice spÃ©cifique
git checkout exercice3

# Voir l'historique d'un exercice
git log --oneline exercice6

# Merger exercice vers master
git checkout master
git merge exercice8 --no-ff
```

## ğŸ§ª Tests & Validation

### Tests AutomatisÃ©s
Chaque exercice inclut des scripts de test:
- `test-*.ps1` pour validation fonctionnelle
- GÃ©nÃ©ration automatique de donnÃ©es test
- VÃ©rification end-to-end du pipeline

### Validation Manuelle
```bash
# Test exercice 3 (weather producer)
cd exercices\exercice3
.\test-weather.ps1

# Test exercice 8 (visualizations)
cd exercices\exercice8
.\test-simple.ps1
```

## ğŸ“‹ Troubleshooting

### ProblÃ¨mes FrÃ©quents

**Kafka ne dÃ©marre pas**
```bash
# Nettoyer les logs
rmdir /S C:\tmp\kraft-combined-logs
.\start-kafka.bat
```

**Erreur Python dependencies**
```bash
# RÃ©installer dans environnement propre
python -m venv kafka-env
kafka-env\Scripts\activate
pip install -r requirements.txt
```

**Spark timeout/memory**
```bash
# Ajuster configuration Spark
export SPARK_CONF_DIR=./spark-conf
# Ã‰diter spark-defaults.conf
```

### Logs Utiles
- **Kafka**: `./kafka_2.13-3.9.1/logs/server.log`
- **Spark**: Console output avec niveau INFO
- **Python**: Logs configurÃ©s par module

## ğŸš€ Extensions Futures

### AmÃ©liorations Techniques
- [ ] **Kubernetes deployment** avec Helm charts
- [ ] **Schema Registry** pour Ã©volution schemas
- [ ] **Kafka Streams** pour processing natif
- [ ] **Monitoring** Prometheus + Grafana

### FonctionnalitÃ©s Business
- [ ] **Machine Learning** prÃ©dictions mÃ©tÃ©o
- [ ] **Alertes temps rÃ©el** (webhooks, email)
- [ ] **APIs REST** pour accÃ¨s donnÃ©es
- [ ] **Interface web** React/Vue.js

### ScalabilitÃ©
- [ ] **Multi-datacenter** replication
- [ ] **Auto-scaling** basÃ© sur charge
- [ ] **Data partitioning** avancÃ©
- [ ] **Compression** et archivage automatique

## ğŸ‘¥ Contribution

### Standards Code
- **Python**: PEP 8, type hints
- **PowerShell**: Verb-Noun naming
- **Git**: Conventional commits
- **Documentation**: README par exercice

### Structure Commits
```
type(scope): description

âœ¨ feat(exercice6): gÃ©ocodage API Open-Meteo
ğŸ› fix(exercice4): schema Spark nullable fields
ğŸ“š docs(README): installation Windows
â™»ï¸ refactor(exercice3): error handling retry
```

## ğŸ“„ Licence

MIT License - Libre d'utilisation pour projets Ã©ducatifs et commerciaux.

## ğŸ†˜ Support

Pour questions ou problÃ¨mes:
1. VÃ©rifier la documentation exercice spÃ©cifique
2. Consulter les logs Kafka/Spark
3. Tester avec donnÃ©es exemples fournies
4. Ouvrir issue avec contexte dÃ©taillÃ©

---

**ğŸ‰ Projet terminÃ© avec succÃ¨s!**
*Pipeline complet Kafka â†’ Spark â†’ HDFS â†’ Analytics opÃ©rationnel*